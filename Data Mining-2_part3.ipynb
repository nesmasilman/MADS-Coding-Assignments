{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8af5acf34d8dca7a45970aeb3179f718",
     "grade": false,
     "grade_id": "cell-6a9a29c5c44fdc17",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"v2.2.033020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5a42420ae9b79f0f14de5be3acc3e6d6",
     "grade": false,
     "grade_id": "cell-2a273215a1273e4c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2: Mining Itemsets (Part III)\n",
    "\n",
    "## Apriori Algorithm under the Hood\n",
    "\n",
    "In part III of the assignment, we are going to continue our exploration in mining frequent itemsets. Specifically, we are going to examine a few key steps in the Apriori algorithm.\n",
    "\n",
    "First, let's import the packages and dependencies that will be used in this part of the assignment. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "071bf5aaa08eceb00ba1d73698dd7fb7",
     "grade": false,
     "grade_id": "cell-40ab1177e0930fe3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ade97c9e1ed24334d0bfb0537002085",
     "grade": false,
     "grade_id": "cell-f600b4432bf9b05f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**<span style=\"color:red\">NOTE: These are all the imports we need to make for this assignment (Part III). You should not make other imports in your submitted notebook. You will receive 0 points for the exercises if your solution includes additional imports.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e05ddf3e4e099e2cd085de10f0470181",
     "grade": false,
     "grade_id": "cell-83dd6259aa6c1c56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### A critical step of the Apriori algorithm is *candidate generation*. That is, candidate *(k+1)*-itemsets should be generated from frequent *k*-itemsets. In the following exercise, we want you to generate candidate 3-itemsets based on the frequent 2-itemsets.\n",
    "\n",
    "### Exercise 3 (25 pts)\n",
    "\n",
    "**Candidate Generation** (20 pts)\n",
    "\n",
    "You will need to complete the `generate_candidate_3_itemsets` function, which takes in a list of frequent 2-itemsets and returns a list of the candidate 3-itemsets (\"candidate\" means that they may or may not be frequent). Please represent an itemset as a  `set` in Python. Make sure that for each candidate 3-itemset in your returned list, **at least one** of its size-2 subset is a frequent 2-itemset, and your list does not contain duplicated itemsets.\n",
    "\n",
    "We have prepared the frequent 2-itemsets for you, which we will help you load from the file named `food_emoji_frequent_2_itemsets.csv`. We will evaluate your function by feeding in the loaded 2-itemsets and examining the return value. Note that the loaded 2-itemsets are different from what you will get with the `emoji_frequent_itemsets` function you implemented in Part II, as we have eliminated the drink-related emojis to make the exercise more trackable.\n",
    "\n",
    "You will receive 20 points if every candidate 3-itemset in your returned list is a **superset of at least one** frequent 2-itemset, every 3-itemset that has a frequent size-2 subset is already in your list, and your list does not contain duplicated sets. \n",
    "\n",
    "Note that this pruning procedure won't give us the smallest set of candidates. In the following challenge exercise, we will further prune the candidate itemsets.\n",
    "\n",
    "**Challenge Exercise** (5 pts)\n",
    "\n",
    "We can further prune the candidate itemsets by requiring **all** size-2 subsets of each candidate  3-itemset to be frequent. \n",
    "\n",
    "Think about the example you've seen in the lecture. Suppose {🍺, 🍼}, {🍺, 🍋}, {🍼, 🍭}, {🍼, 🍋} are all the frequent 2-itemsets. In the lecture, we said {🍺, 🍼, 🍋}, {🍺, 🍼, 🍭}, {🍼, 🍭, 🍋} are candidate 3-itemsets because each 3-itemset is a superset of **at least one** frequent 2-itemset. However, a larger itemset can never be frequent as long as one of its subset is not frequent. In this case, {🍺, 🍼, 🍭} can never been frequent because {🍺, 🍭} is not frequent. Neither can {🍼, 🍭, 🍋}, as the subset {🍭, 🍋} is not frequent. Ideally, we should be able to exclude the two candidate 3-itemsets {🍺, 🍼, 🍭} and {🍼, 🍭, 🍋} even without scanning the database for counting.\n",
    "\n",
    "For this challenge exercise, please prune your generated candidate 3-itemsets by requiring all their subsets to be frequent. You will receive 5 points if the pruning is done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a4eb5efc9950490731e5bb803951aa01",
     "grade": false,
     "grade_id": "cell-b571996c4b5cf20c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_candidate_3_itemsets(base_itemsets):\n",
    "#    candidates = []\n",
    "#    candidate_items = set()\n",
    "#    for itemset in base_itemsets:\n",
    "#        for i in itemset:\n",
    "#            candidate_items.add(i)\n",
    "\n",
    "    ll=pd.DataFrame(base_itemsets, columns=['one', 'two'])\n",
    "    ll.sort_values(by='one', inplace=True, ascending=False)\n",
    "    uni=ll['one'].unique()\n",
    "    zz=[]\n",
    "    for num in range(0,len(uni),1):\n",
    "        df=ll[ll['one']==uni[num]]\n",
    "        uniq=df['two'].unique()\n",
    "        for each in range(0,len(df),1):\n",
    "            qq1=list(df.iloc[each])\n",
    "            for item in uniq:\n",
    "                if item not in qq1:\n",
    "                        zz.append([qq1[0],qq1[1],item])\n",
    "    new=zz[:]\n",
    "    newdf=pd.DataFrame(new, columns=['one','two','three'])\n",
    "    newdf['four']=newdf['one']\n",
    "    newdf['five']=newdf['two']\n",
    "    newdf['six']=newdf['three']\n",
    "    nl=[]\n",
    "    org=[]\n",
    "    for raw in range(0,len(newdf),1):\n",
    "            bb= list(newdf.iloc[raw,3:].sort_values(inplace=False))\n",
    "            if bb not in nl:\n",
    "                nl.append(bb)\n",
    "                org.append(newdf.iloc[raw])\n",
    "\n",
    "    data=pd.DataFrame(org).iloc[:,0:3]\n",
    "    inter=[]\n",
    "    for raw in range(0,len(data),1):\n",
    "        sett=set(data.iloc[raw])\n",
    "\n",
    "        inter.append(sett)\n",
    "\n",
    "    three=pd.DataFrame(inter, columns=['one','two','three'])\n",
    "\n",
    "    three\n",
    "    item1=[]\n",
    "    item2=[]\n",
    "    item3=[]\n",
    "    item4=[]\n",
    "    for line in inter:\n",
    "        gr1=set((list(line)[1],list(line)[2]))\n",
    "        gr2=set((list(line)[2],list(line)[0]))\n",
    "        gr3=set((list(line)[1],list(line)[0]))\n",
    "        gr4=set((list(line)[2],list(line)[1],list(line)[0]))\n",
    "        item1.append(gr1)\n",
    "        item2.append(gr2)\n",
    "        item3.append(gr3)\n",
    "        item4.append(gr4)\n",
    "    #list(generate_candidate_3_itemsets(frequent_2_itemsets)[0])\n",
    "    three['list_one']=item1\n",
    "    three['list_two']=item2\n",
    "    three['list_three']=item3\n",
    "    three\n",
    "\n",
    "    def check(aset):\n",
    "        if aset in base_itemsets:\n",
    "            ans=1\n",
    "        else:\n",
    "            ans=0\n",
    "        return ans\n",
    "    three['status_1']=three['list_one'].apply(check)\n",
    "    three['status_2']=three['list_two'].apply(check)\n",
    "    three['status_3']=three['list_three'].apply(check)\n",
    "    three['sum']=three['status_1']+three['status_2']+three['status_3']\n",
    "    three['all']=item4\n",
    "    result=three[three['sum']==3]\n",
    "\n",
    "    def another_sort(lst):\n",
    "        return sorted(list(lst))\n",
    "\n",
    "    three['sorted']=three['all'].apply(another_sort)\n",
    "    off=[]\n",
    "    for item in three['all']:\n",
    "        if item not in off:\n",
    "            off.append(item)\n",
    "    candidates=list(result['all'])\n",
    "    return candidates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #    raise NotImplementedError()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_2_itemsets = []\n",
    "with open('assets/food_emoji_frequent_2_itemsets.csv', 'r') as fin:\n",
    "    for line in fin:\n",
    "        frequent_2_itemsets.append(set([line[0], line[1]]))\n",
    "\n",
    "candidate_3_itemsets = generate_candidate_3_itemsets(frequent_2_itemsets)\n",
    "# You can uncomment the following line to preview the generated candidate itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b35cd025f3b1790398cdd0b021b6592a",
     "grade": true,
     "grade_id": "cell-ac6353d3709310e2",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This code block tests whether the `generate_candidate_3_itemsets` function is implemented correctly.\n",
    "# We hide some tests, so passing all the displayed assertions does not guarantee full points.\n",
    "\n",
    "# load from file\n",
    "frequent_2_itemsets = []\n",
    "with open('assets/food_emoji_frequent_2_itemsets.csv', 'r') as fin:\n",
    "    for line in fin:\n",
    "        frequent_2_itemsets.append(set([line[0], line[1]]))\n",
    "\n",
    "# obtain return value\n",
    "candidate_3_itemsets = generate_candidate_3_itemsets(frequent_2_itemsets)\n",
    "\n",
    "# assertions\n",
    "examined = []\n",
    "for candidate in candidate_3_itemsets:\n",
    "    assert len(candidate) == 3, f\"[Exercise 3] Candidate {candidate} not a 3-itemset\"\n",
    "    assert candidate not in examined, f\"[Exercise 3] Duplicate 3-itemsets: {candidate}\"\n",
    "    emoji1, emoji2, emoji3 = list(candidate)\n",
    "    assert (set([emoji1, emoji2]) in frequent_2_itemsets or \n",
    "                set([emoji1, emoji3]) in frequent_2_itemsets or\n",
    "                set([emoji2, emoji3]) in frequent_2_itemsets),\\\n",
    "        f\"[Exercise 3] Candidate {candidate} does not contain frequent 2-itemsets\"\n",
    "    examined.append(candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ccc35ea3f115a2cadd8a1e6a3a1d8a62",
     "grade": true,
     "grade_id": "cell-46d1b3539efe8f13",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This code block tests whether the `generate_candidate_3_itemsets` function can perform the \n",
    "# additional pruning for the challenge exercise.\n",
    "# We hide some tests, so passing all the displayed assertions does not guarantee full points.\n",
    "\n",
    "# load from file\n",
    "frequent_2_itemsets = []\n",
    "with open('assets/food_emoji_frequent_2_itemsets.csv', 'r') as fin:\n",
    "    for line in fin:\n",
    "        frequent_2_itemsets.append(set([line[0], line[1]]))\n",
    "\n",
    "# obtain the return value\n",
    "candidate_3_itemsets = generate_candidate_3_itemsets(frequent_2_itemsets)\n",
    "\n",
    "# assertions\n",
    "examined = []\n",
    "for candidate in candidate_3_itemsets:\n",
    "    if len(candidate) != 3:\n",
    "        assert False, f\"[Exercise 3 (Challenge Exercise)] Candidate {candidate} not a 3-itemset\"\n",
    "        break\n",
    "    assert candidate not in examined, f\"[Exercise 3 (Challenge Exercise)] Duplicate 3-itemsets: {candidate}\"\n",
    "    emoji1, emoji2, emoji3 = list(candidate)\n",
    "    assert (set([emoji1, emoji2]) in frequent_2_itemsets and \n",
    "                set([emoji1, emoji3]) in frequent_2_itemsets and\n",
    "                set([emoji2, emoji3]) in frequent_2_itemsets),\\\n",
    "        f\"[Exercise 3 (Challenge Exercise)] Candidate {candidate} contains infrequent 2-itemsets\"\n",
    "    examined.append(candidate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31c7a41b3dbebc24065c6db894ae15e7",
     "grade": false,
     "grade_id": "cell-caa39a52dec4ed54",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Before heading to the next exercise, please run the following code block to load the data set prepared in Part I of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c36a6b88a2b94593006fa97f554c0a8",
     "grade": false,
     "grade_id": "cell-708c9c2375c85bb5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"assets/food_drink_emoji_tweets.txt\", sep=\"\\t\", header=None)\n",
    "tweets_df.columns = ['text']\n",
    "\n",
    "emoji_list = \"🍇🍈🍉🍊🍋🍌🍍🥭🍎🍏🍐🍑🍒🍓🥝🍅🥥🥑🍆🥔🥕🌽🌶🥒🥬🥦🍄🥜🌰🍞🥐🥖🥨🥯🥞🧀🍖🍗🥩🥓🍔🍟🍕🌭🥪🌮🌯🥙🥚🍳🥘🍲🥣🥗🍿🧂🥫🍱🍘🍙🍚🍛🍜🍝🍠🍢🍣🍤🍥🥮🍡🥟🥠🥡🦀🦞🦐🦑🍦🍧🍨🍩🍪🎂🍰🧁🥧🍫🍬🍭🍮🍯🍼🥛☕🍵🍶🍾🍷🍸🍹🍺🍻🥂🥃\"\n",
    "emoji_set = set(emoji_list)\n",
    "\n",
    "def extract_uniq_emojis(text):\n",
    "    return \n",
    "\n",
    "tweets_df['emojis'] = tweets_df.text.apply(lambda text:np.unique([chr for chr in text if chr in emoji_set]))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "emoji_matrix = pd.DataFrame(data=mlb.fit_transform(tweets_df.emojis), index=tweets_df.index, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "523fa31841298c6c387c84473f062f23",
     "grade": false,
     "grade_id": "cell-a96a8b8a03c31d55",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Exercise 4 (15 pts)\n",
    "\n",
    "With the candidate itemsets ready, the final step in one iteration of the Apriori algorithm is to scan the database (in our case, you can either scan the `emoji_matrix` or `tweets_df`) and count the occurrence of each candidate itemset, divide it by the total number of records to derive the support, and output candidate itemsets whose support meets the threshold. \n",
    "\n",
    "Please complete the `calculate_frequent_itemsets` function below. The input (`candidate_itemsets`) is a list of candidate 3-itemsets. Your function should return a complete list of frequent 3-itemsets with a minimal support of `min_support`. The returned list should not contain duplicated itemsets. The order of the list does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3f6de8c15d720a2942e9089241eb570d",
     "grade": false,
     "grade_id": "cell-cd933ca0b3477c01",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_frequent_itemsets(candidate_itemsets, min_support=0.005):\n",
    "    \n",
    "#        raise NotImplementedError()\n",
    "    \n",
    "    ttt=tweets_df['emojis']\n",
    "    twt=[]\n",
    "    for one in ttt:\n",
    "        twt.append(set(one))\n",
    "\n",
    "    candidate_itemsets = generate_candidate_3_itemsets(frequent_2_itemsets)\n",
    "\n",
    "    fdf=pd.DataFrame(candidate_itemsets, columns=['one','two','three'])\n",
    "    items1=[]\n",
    "    items2=[]\n",
    "    items3=[]\n",
    "    items4=[]\n",
    "\n",
    "    for line in range(0,len(fdf),1):\n",
    "        gr1=set((fdf.iloc[line,0],fdf.iloc[line,1]))\n",
    "        gr2=set((fdf.iloc[line,0],fdf.iloc[line,2]))\n",
    "        gr3=set((fdf.iloc[line,1],fdf.iloc[line,2]))\n",
    "        gr4=set((fdf.iloc[line,0],fdf.iloc[line,1],fdf.iloc[line,2]))\n",
    "        items1.append(gr1)\n",
    "        items2.append(gr2)\n",
    "        items3.append(gr3)\n",
    "        items4.append(gr4)\n",
    "\n",
    "    fdf['list_one']=items1\n",
    "    fdf['list_two']=items2\n",
    "    fdf['list_three']=items3\n",
    "    fdf['alll']=items4\n",
    "    def check(aset):\n",
    "        count=0\n",
    "        for tt in twt:\n",
    "            if aset.issubset(tt):\n",
    "                count+=1\n",
    "        return (count/len(twt))\n",
    "\n",
    "    fdf['status_4']=fdf['alll'].apply(check)\n",
    "    \n",
    "    frequent_itemsets=list(fdf[fdf['status_4']>=min_support]['alll'])\n",
    "    return frequent_itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dea66e6a2822ebc3d087189110967614",
     "grade": false,
     "grade_id": "cell-10d4c87d1e53e34b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'🍉', '🍍', '🥝'},\n",
       " {'🍊', '🍍', '🥝'},\n",
       " {'🍇', '🍍', '🥝'},\n",
       " {'🍉', '🍊', '🥝'},\n",
       " {'🍇', '🍉', '🥝'},\n",
       " {'🍇', '🍊', '🥝'},\n",
       " {'🍫', '🍬', '🍭'},\n",
       " {'🍫', '🍰', '🎂'},\n",
       " {'🍦', '🍧', '🍨'},\n",
       " {'🍔', '🍕', '🍟'},\n",
       " {'🍉', '🍊', '🍍'},\n",
       " {'🍇', '🍊', '🍍'},\n",
       " {'🍇', '🍉', '🍍'},\n",
       " {'🍇', '🍉', '🍊'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_frequent_itemsets(candidate_3_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c11b67d529ddeaf2413eeb4950513fc5",
     "grade": true,
     "grade_id": "cell-876b8dee833fac6f",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This code block tests whether the `calculate_frequent_itemsets` function work as expected.\n",
    "# We hide some tests, so passing all the displayed assertions does not guarantee full points.\n",
    "\n",
    "answer = calculate_frequent_itemsets(candidate_3_itemsets)\n",
    "\n",
    "assert len(answer) == 14\n",
    "assert {'🍇', '🍉', '🍊'} in answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_data_mining_i_v2_assignment2_part3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
